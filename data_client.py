from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

from langchain_mcp_adapters.tools import load_mcp_tools
from langchain_mcp_adapters.prompts import load_mcp_prompt
from langgraph.prebuilt import create_react_agent
from langchain_openai import ChatOpenAI

from langchain_ollama import ChatOllama

from dotenv import load_dotenv
import os

#load_dotenv()
#os.getenv("OPENAI_API_KEY")
OLLAMA_SERVER = "http://192.168.2.209:11434"
MODEL_NAME = "qwen2.5:72b"


#model = ChatOpenAI(model=MODEL_NAME)
model = ChatOllama(model=MODEL_NAME, base_url=OLLAMA_SERVER,temperature=0,num_ctx=8192)
SERVER_VERSION = "v.3.0" 

server_params = StdioServerParameters(
    command="python",
    args=[f"./data_server_{SERVER_VERSION}.py"],
)



async def run():
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            await session.initialize()

            ##### AGENT #####
            tools = await load_mcp_tools(session)
            agent = create_react_agent(model, tools)

            # âœ… ëŒ€í™” ížˆìŠ¤í† ë¦¬ ê´€ë¦¬
            conversation_history = []
            system_prompt = await load_mcp_prompt(
                session, "default_prompt", arguments={"message": ""}
            )
            # ì‹œìŠ¤í…œ ë©”ì‹œì§€ë§Œ ì¶”ì¶œ (ì²« ë²ˆì§¸ ë©”ì‹œì§€)
            if system_prompt:
                conversation_history.append(system_prompt[0])

            print("\n" + "="*60)
            print(f" MCP ë°ì´í„° ë¶„ì„ ì‹œìŠ¤í…œ (v.3.0) - Model: {MODEL_NAME}")
            print("="*60)
            print("Tip: ì´ì „ ëŒ€í™”ë¥¼ ê¸°ì–µí•©ë‹ˆë‹¤. ìžì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ì„¸ìš”!")
            print(" ì˜ˆ: 'ì´ì œ ì´ìƒì¹˜ë¥¼ ì œê±°í•´ì¤˜', 'ê·¸ ê²°ê³¼ë¥¼ ì‹œê°í™”í•´ì¤˜'")
            print(" Commands: 'clear' - ëŒ€í™” ì´ˆê¸°í™”, 'exit/ì¢…ë£Œ' - ì¢…ë£Œ")
            print("="*60 + "\n")

            while True:
                ##### REQUEST & RESPOND #####
                try:
                    user_input = input("You: ")
                    
                    if user_input.lower() in ["exit", "quit", "q", "ì¢…ë£Œ"]:
                        print("\nðŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
                        break
                    
                    # ëŒ€í™” ì´ˆê¸°í™” ëª…ë ¹
                    if user_input.lower() == "clear":
                        conversation_history = [conversation_history[0]]  # ì‹œìŠ¤í…œ ë©”ì‹œì§€ë§Œ ìœ ì§€
                        print("\nðŸ”„ ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.\n")
                        continue
                    
                    if not user_input.strip():
                        continue

                    # ì‚¬ìš©ìž ë©”ì‹œì§€ë¥¼ ížˆìŠ¤í† ë¦¬ì— ì¶”ê°€
                    from langchain_core.messages import HumanMessage
                    conversation_history.append(HumanMessage(content=user_input))

                    print("\nðŸ¤” ë¶„ì„ ì¤‘...\n")
                    
                    # ì „ì²´ ëŒ€í™” ížˆìŠ¤í† ë¦¬ë¥¼ agentì— ì „ë‹¬
                    response = await agent.ainvoke({"messages": conversation_history})
                    
                    # AI ì‘ë‹µì„ ížˆìŠ¤í† ë¦¬ì— ì¶”ê°€
                    conversation_history = response["messages"]
                    
                    # ìµœì‹  ì‘ë‹µ ì¶œë ¥
                    ai_response = response["messages"][-1].content
                    print("="*60)
                    print("AI:", ai_response)
                    print("="*60 + "\n")

                except EOFError:
                    print("\nðŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                except KeyboardInterrupt:
                    print("\n\nðŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                except Exception as e:
                    print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n")
                    # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë§ˆì§€ë§‰ ì‚¬ìš©ìž ë©”ì‹œì§€ ì œê±°
                    if len(conversation_history) > 1:
                        conversation_history.pop()


import asyncio

asyncio.run(run())
